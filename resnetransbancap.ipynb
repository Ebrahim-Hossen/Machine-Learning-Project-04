{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":3600745,"sourceType":"datasetVersion","datasetId":2159203},{"sourceId":3674436,"sourceType":"datasetVersion","datasetId":2199252}],"dockerImageVersionId":30554,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nfrom numpy import array\nfrom pickle import load\n\nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport sys, time, os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom keras.utils import pad_sequences\nfrom keras.utils import load_img\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import img_to_array\n#from keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:38.115615Z","iopub.execute_input":"2023-09-21T02:11:38.115909Z","iopub.status.idle":"2023-09-21T02:11:56.184743Z","shell.execute_reply.started":"2023-09-21T02:11:38.115871Z","shell.execute_reply":"2023-09-21T02:11:56.183769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = \"/kaggle/input/flickr8k/Images\"\ndata = pd.read_csv(\"/kaggle/input/bancap/BAN-Cap_captiondata.csv\")\njpgs = os.listdir(image_path)\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))\nprint(type(data))\ndata","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.187547Z","iopub.execute_input":"2023-09-21T02:11:56.188587Z","iopub.status.idle":"2023-09-21T02:11:56.671686Z","shell.execute_reply.started":"2023-09-21T02:11:56.188552Z","shell.execute_reply":"2023-09-21T02:11:56.670717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['caption_id']=data['caption_id'].str[:-2]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.673262Z","iopub.execute_input":"2023-09-21T02:11:56.673931Z","iopub.status.idle":"2023-09-21T02:11:56.700579Z","shell.execute_reply.started":"2023-09-21T02:11:56.673896Z","shell.execute_reply":"2023-09-21T02:11:56.699584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=data.drop('english_caption',axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.703480Z","iopub.execute_input":"2023-09-21T02:11:56.703882Z","iopub.status.idle":"2023-09-21T02:11:56.720919Z","shell.execute_reply.started":"2023-09-21T02:11:56.703850Z","shell.execute_reply":"2023-09-21T02:11:56.720040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.rename(columns = {'caption_id':'filename','bengali_caption':'caption'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.722515Z","iopub.execute_input":"2023-09-21T02:11:56.722913Z","iopub.status.idle":"2023-09-21T02:11:56.735390Z","shell.execute_reply.started":"2023-09-21T02:11:56.722882Z","shell.execute_reply":"2023-09-21T02:11:56.734530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uni_filenames = np.unique(data.filename.values)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.736245Z","iopub.execute_input":"2023-09-21T02:11:56.736493Z","iopub.status.idle":"2023-09-21T02:11:56.798173Z","shell.execute_reply.started":"2023-09-21T02:11:56.736471Z","shell.execute_reply":"2023-09-21T02:11:56.796924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import font_manager\nbangla_font_path = \"/kaggle/input/synth-indic-custom-resources/SYNTH_INDIC/fonts/bangla/AdorshoLipi_20-07-2007.ttf\"  # Provide the path to your installed Bangla font\nbangla_font_prop = font_manager.FontProperties(fname=bangla_font_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.801761Z","iopub.execute_input":"2023-09-21T02:11:56.802073Z","iopub.status.idle":"2023-09-21T02:11:56.810268Z","shell.execute_reply.started":"2023-09-21T02:11:56.802047Z","shell.execute_reply":"2023-09-21T02:11:56.808826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"npic = 5\nnpix = 224\ntarget_size = (npix,npix,3)\ncount = 1\n\nfig = plt.figure(figsize=(10,20))\nfor jpgfnm in uni_filenames[10:15]:\n    filename = image_path + '/' + jpgfnm\n    captions = list(data[\"caption\"].loc[data[\"filename\"]==jpgfnm].values)\n    image_load = load_img(filename, target_size=target_size)\n    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n    ax.imshow(image_load)\n    count += 1\n\n    ax = fig.add_subplot(npic,2,count)\n    plt.axis('off')\n    ax.plot()\n    ax.set_xlim(0,1)\n    ax.set_ylim(0,len(captions))\n    for i, caption in enumerate(captions):\n        ax.text(0,i,caption,fontproperties=bangla_font_prop)\n    count += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:56.813784Z","iopub.execute_input":"2023-09-21T02:11:56.814157Z","iopub.status.idle":"2023-09-21T02:11:58.038342Z","shell.execute_reply.started":"2023-09-21T02:11:56.814130Z","shell.execute_reply":"2023-09-21T02:11:58.037065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = []\nfor txt in data.caption.values:\n    vocabulary.extend(txt.split())\nprint('Vocabulary Size: %d' % len(set(vocabulary)))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:58.039447Z","iopub.execute_input":"2023-09-21T02:11:58.039766Z","iopub.status.idle":"2023-09-21T02:11:58.144587Z","shell.execute_reply.started":"2023-09-21T02:11:58.039738Z","shell.execute_reply":"2023-09-21T02:11:58.143457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/working/images/\"\nall_captions = []\nfor caption  in data['caption']:\n    caption = '<start> ' + caption+ ' <end>'\n    all_captions.append(caption)\n\nall_captions[:10]\nif not os.path.exists(PATH):\n    os.makedirs(PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:58.150006Z","iopub.execute_input":"2023-09-21T02:11:58.152311Z","iopub.status.idle":"2023-09-21T02:11:58.189332Z","shell.execute_reply.started":"2023-09-21T02:11:58.152281Z","shell.execute_reply":"2023-09-21T02:11:58.188350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:58.190865Z","iopub.execute_input":"2023-09-21T02:11:58.191242Z","iopub.status.idle":"2023-09-21T02:11:58.195528Z","shell.execute_reply.started":"2023-09-21T02:11:58.191199Z","shell.execute_reply":"2023-09-21T02:11:58.194643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_img_name_vector = []\nfor annot in data[\"filename\"]:\n    filename = annot\n    source_path = '/kaggle/input/flickr8k/Images/' + filename\n    destination_path = '/kaggle/working/images/' + filename\n    shutil.copyfile(source_path, destination_path)\n    \n    full_image_path = PATH + annot\n    all_img_name_vector.append(full_image_path)\n    \nall_img_name_vector[:10]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:11:58.197198Z","iopub.execute_input":"2023-09-21T02:11:58.197948Z","iopub.status.idle":"2023-09-21T02:13:59.358266Z","shell.execute_reply.started":"2023-09-21T02:11:58.197913Z","shell.execute_reply":"2023-09-21T02:13:59.357226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\nprint(f\"len(all_captions) : {len(all_captions)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:13:59.359878Z","iopub.execute_input":"2023-09-21T02:13:59.360480Z","iopub.status.idle":"2023-09-21T02:13:59.366019Z","shell.execute_reply.started":"2023-09-21T02:13:59.360443Z","shell.execute_reply":"2023-09-21T02:13:59.365059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_limiter(num,total_captions,all_img_name_vector):\n    train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n    train_captions = train_captions[:num]\n    img_name_vector = img_name_vector[:num]\n    return train_captions,img_name_vector\n\ntrain_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:13:59.367241Z","iopub.execute_input":"2023-09-21T02:13:59.368030Z","iopub.status.idle":"2023-09-21T02:13:59.405249Z","shell.execute_reply.started":"2023-09-21T02:13:59.367997Z","shell.execute_reply":"2023-09-21T02:13:59.404322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (224, 224))\n    img = preprocess_input(img)\n    return img, image_path\n\nimage_model = ResNet50(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n\nimage_features_extract_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:13:59.406432Z","iopub.execute_input":"2023-09-21T02:13:59.408732Z","iopub.status.idle":"2023-09-21T02:14:08.877042Z","shell.execute_reply.started":"2023-09-21T02:13:59.408699Z","shell.execute_reply":"2023-09-21T02:14:08.876212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:14:08.878268Z","iopub.execute_input":"2023-09-21T02:14:08.878645Z","iopub.status.idle":"2023-09-21T02:14:09.192479Z","shell.execute_reply.started":"2023-09-21T02:14:08.878609Z","shell.execute_reply":"2023-09-21T02:14:09.191452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor img, path in tqdm(image_dataset):\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,\n                             (batch_features.shape[0], -1, batch_features.shape[3]))\n\n    for bf, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        np.save(path_of_feature, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:14:09.194145Z","iopub.execute_input":"2023-09-21T02:14:09.194807Z","iopub.status.idle":"2023-09-21T02:15:16.914613Z","shell.execute_reply.started":"2023-09-21T02:14:09.194773Z","shell.execute_reply":"2023-09-21T02:15:16.913662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length=25","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:16.916120Z","iopub.execute_input":"2023-09-21T02:15:16.917155Z","iopub.status.idle":"2023-09-21T02:15:16.922173Z","shell.execute_reply.started":"2023-09-21T02:15:16.917118Z","shell.execute_reply":"2023-09-21T02:15:16.921018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_k = 12000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                 oov_token=\"<unk>\",\n                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\n\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post',maxlen=max_length)\n#Let’s visualize the padded training and captions and the tokenized vectors:\ntrain_captions[:3]\nprint(cap_vector[0])\nprint(len(cap_vector[0]))\ntrain_captions[:3]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:16.923542Z","iopub.execute_input":"2023-09-21T02:15:16.924080Z","iopub.status.idle":"2023-09-21T02:15:19.593120Z","shell.execute_reply.started":"2023-09-21T02:15:16.924047Z","shell.execute_reply":"2023-09-21T02:15:19.592055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seqs[:3]","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.594971Z","iopub.execute_input":"2023-09-21T02:15:19.595706Z","iopub.status.idle":"2023-09-21T02:15:19.604717Z","shell.execute_reply.started":"2023-09-21T02:15:19.595669Z","shell.execute_reply":"2023-09-21T02:15:19.603545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(cap_vector))\nprint(len(cap_vector))\nprint(cap_vector.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.607455Z","iopub.execute_input":"2023-09-21T02:15:19.607806Z","iopub.status.idle":"2023-09-21T02:15:19.619147Z","shell.execute_reply.started":"2023-09-21T02:15:19.607777Z","shell.execute_reply":"2023-09-21T02:15:19.616225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_max_length(tensor):\n    return max(len(t) for t in tensor)\nmax_length = calc_max_length(train_seqs)\nmax_length=25\ndef calc_min_length(tensor):\n    return min(len(t) for t in tensor)\nmin_length = calc_min_length(train_seqs)\n\nprint('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.621542Z","iopub.execute_input":"2023-09-21T02:15:19.622675Z","iopub.status.idle":"2023-09-21T02:15:19.639530Z","shell.execute_reply.started":"2023-09-21T02:15:19.622618Z","shell.execute_reply":"2023-09-21T02:15:19.638489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.02, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.641155Z","iopub.execute_input":"2023-09-21T02:15:19.641492Z","iopub.status.idle":"2023-09-21T02:15:19.679677Z","shell.execute_reply.started":"2023-09-21T02:15:19.641461Z","shell.execute_reply":"2023-09-21T02:15:19.678607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nnum_steps = len(img_name_train) // BATCH_SIZE\n\ndef map_func(img_name, cap):\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n    return img_tensor, cap\n\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.681131Z","iopub.execute_input":"2023-09-21T02:15:19.681463Z","iopub.status.idle":"2023-09-21T02:15:19.869348Z","shell.execute_reply.started":"2023-09-21T02:15:19.681431Z","shell.execute_reply":"2023-09-21T02:15:19.868304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#num_steps = len(img_name_val) // BATCH_SIZE\nval_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\nval_dataset = val_dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.870764Z","iopub.execute_input":"2023-09-21T02:15:19.871572Z","iopub.status.idle":"2023-09-21T02:15:19.918463Z","shell.execute_reply.started":"2023-09-21T02:15:19.871533Z","shell.execute_reply":"2023-09-21T02:15:19.917434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding_1d(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                           np.arange(d_model)[np.newaxis, :],\n                           d_model)\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef positional_encoding_2d(row,col,d_model):\n    assert d_model % 2 == 0\n    row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n    col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n\n    angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n    angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n\n    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n    pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.919931Z","iopub.execute_input":"2023-09-21T02:15:19.920415Z","iopub.status.idle":"2023-09-21T02:15:19.935045Z","shell.execute_reply.started":"2023-09-21T02:15:19.920380Z","shell.execute_reply":"2023-09-21T02:15:19.934016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_pos_encoding_1 = positional_encoding_1d(top_k+1, 512)\n\nplt.pcolormesh(sample_pos_encoding_1.numpy()[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:19.936751Z","iopub.execute_input":"2023-09-21T02:15:19.937132Z","iopub.status.idle":"2023-09-21T02:15:24.525142Z","shell.execute_reply.started":"2023-09-21T02:15:19.937098Z","shell.execute_reply":"2023-09-21T02:15:24.524026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_pos_encoding_2 = positional_encoding_2d(8,8,512)\n\nplt.pcolormesh(sample_pos_encoding_2.numpy()[0], cmap='RdBu')\nplt.xlabel('Depth')\nplt.xlim((0, 512))\nplt.ylabel('Position')\nplt.colorbar()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:24.535798Z","iopub.execute_input":"2023-09-21T02:15:24.536943Z","iopub.status.idle":"2023-09-21T02:15:25.132970Z","shell.execute_reply.started":"2023-09-21T02:15:24.536906Z","shell.execute_reply":"2023-09-21T02:15:25.132007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9) \n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask=None):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                 (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n                tf.keras.layers.Dense(dff, activation='relu'),tf.keras.layers.Dense(d_model)])# (batch_size, seq_len, d_model)])","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.134449Z","iopub.execute_input":"2023-09-21T02:15:25.135023Z","iopub.status.idle":"2023-09-21T02:15:25.152091Z","shell.execute_reply.started":"2023-09-21T02:15:25.134972Z","shell.execute_reply":"2023-09-21T02:15:25.151171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.001):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask=None):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n        return out2\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.001):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.155399Z","iopub.execute_input":"2023-09-21T02:15:25.155660Z","iopub.status.idle":"2023-09-21T02:15:25.173214Z","shell.execute_reply.started":"2023-09-21T02:15:25.155637Z","shell.execute_reply":"2023-09-21T02:15:25.172249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, row_size,col_size,rate=0.001):\n        super(Encoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n        self.pos_encoding = positional_encoding_2d(row_size,col_size,self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding,rate=0.001):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                         for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                            look_ahead_mask, padding_mask)\n         \n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n\n        return x, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.174546Z","iopub.execute_input":"2023-09-21T02:15:25.175058Z","iopub.status.idle":"2023-09-21T02:15:25.194241Z","shell.execute_reply.started":"2023-09-21T02:15:25.175021Z","shell.execute_reply":"2023-09-21T02:15:25.193223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size,\n              target_vocab_size,max_pos_encoding, rate=0.001):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n                          target_vocab_size,max_pos_encoding, rate)\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training,look_ahead_mask=None,dec_padding_mask=None,enc_padding_mask=None   ):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model      )\n        dec_output, attention_weights = self.decoder(\n      tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        return final_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.195976Z","iopub.execute_input":"2023-09-21T02:15:25.196358Z","iopub.status.idle":"2023-09-21T02:15:25.212655Z","shell.execute_reply.started":"2023-09-21T02:15:25.196326Z","shell.execute_reply":"2023-09-21T02:15:25.211758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_layer = 8\nd_model = 512\ndff = 2048\nnum_heads = 8\nrow_size = 8\ncol_size = 8\ntarget_vocab_size = top_k + 1\ndropout_rate = 0.001\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)  \n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        #print(tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2))\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\nlearning_rate = CustomSchedule(d_model)\n#learning_rate_value = learning_rate(4000)\n#print(learning_rate_value)\n#print(type(learning_rate))\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                    epsilon=1e-9)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n\ntransformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.214243Z","iopub.execute_input":"2023-09-21T02:15:25.214674Z","iopub.status.idle":"2023-09-21T02:15:25.648146Z","shell.execute_reply.started":"2023-09-21T02:15:25.214642Z","shell.execute_reply":"2023-09-21T02:15:25.647177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transformer.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.649769Z","iopub.execute_input":"2023-09-21T02:15:25.650181Z","iopub.status.idle":"2023-09-21T02:15:25.655430Z","shell.execute_reply.started":"2023-09-21T02:15:25.650128Z","shell.execute_reply":"2023-09-21T02:15:25.654117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_learning_rate = CustomSchedule(d_model=512)\n\nplt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:25.657204Z","iopub.execute_input":"2023-09-21T02:15:25.657556Z","iopub.status.idle":"2023-09-21T02:15:26.053041Z","shell.execute_reply.started":"2023-09-21T02:15:25.657524Z","shell.execute_reply":"2023-09-21T02:15:26.051906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import array_to_img","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:26.054880Z","iopub.execute_input":"2023-09-21T02:15:26.055598Z","iopub.status.idle":"2023-09-21T02:15:26.063380Z","shell.execute_reply.started":"2023-09-21T02:15:26.055534Z","shell.execute_reply":"2023-09-21T02:15:26.062420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate(inputs, targets):\n#     predictions = transformer(inputs, targets, training=False)\n#     loss = loss_function(targets, predictions)\n#     val_loss(loss)\n#     val_accuracy(targets, predictions)\ndef create_masks_decoder(tar):\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    return combined_mask\n@tf.function\ndef train_step(img_tensor, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    dec_mask = create_masks_decoder(tar_inp)\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(img_tensor, tar_inp,True, dec_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)   \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n    \ndef valid_step(img_tensor, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    dec_mask = create_masks_decoder(tar_inp)\n    with tf.GradientTape() as tape:\n        val_predictions, _ = transformer(img_tensor, tar_inp,True, dec_mask)\n        val_loss_value = loss_function(tar_real, val_predictions)\n    val_loss(val_loss_value)\n    val_accuracy(tar_real, val_predictions)\n\ntrain_accuracy_values = []\ntrain_loss_values=[]\nval_accuracy_values = []\nval_loss_values=[]\nfor epoch in range(40):\n    start = time.time()\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    val_loss.reset_states()\n    val_accuracy.reset_states()\n    for (batch, (img_tensor, tar)) in enumerate(dataset):\n        train_step(img_tensor, tar)\n        if batch % 50 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n         epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n    # Validation loop\n    for (batch, (val_img_tensor, val_tar)) in enumerate(val_dataset):\n        valid_step(img_tensor, tar)\n#         if batch % 50 == 0:\n#             print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n#          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n#         val_predictions, _ = transformer(val_img_tensor, val_tar[:, :-1], False, None)\n#         val_loss_value = loss_function(val_tar[:, 1:], val_predictions)\n#         val_loss(val_loss_value)\n#         val_accuracy(val_tar[:, 1:], val_predictions)\n    \n    train_accuracy_values.append(train_accuracy.result())\n    train_loss_values.append(train_loss.result())\n    val_accuracy_values.append(val_accuracy.result())\n    val_loss_values.append(val_loss.result())\n    \n    print('Epoch {} Loss {:.4f} Accuracy {:.4f} Val Loss {:.4f} Val Accuracy {:.4f}'.format(\n        epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result(), val_accuracy.result()))\n    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","metadata":{"execution":{"iopub.status.busy":"2023-09-21T02:15:26.065007Z","iopub.execute_input":"2023-09-21T02:15:26.065360Z","iopub.status.idle":"2023-09-21T04:45:00.432755Z","shell.execute_reply.started":"2023-09-21T02:15:26.065328Z","shell.execute_reply":"2023-09-21T04:45:00.431698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the accuracy values\nnum_epochs=40\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, train_accuracy_values, label='Train Accuracy')\nplt.plot(epochs, val_accuracy_values, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:00.441021Z","iopub.execute_input":"2023-09-21T04:45:00.441380Z","iopub.status.idle":"2023-09-21T04:45:00.778782Z","shell.execute_reply.started":"2023-09-21T04:45:00.441347Z","shell.execute_reply":"2023-09-21T04:45:00.777909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the accuracy values\nnum_epochs=40\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, train_loss_values, label='Train Loss')\nplt.plot(epochs, val_loss_values, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:00.780349Z","iopub.execute_input":"2023-09-21T04:45:00.780712Z","iopub.status.idle":"2023-09-21T04:45:01.088673Z","shell.execute_reply.started":"2023-09-21T04:45:00.780670Z","shell.execute_reply":"2023-09-21T04:45:01.087769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate(image):\n#     temp_input = tf.expand_dims(load_image(image)[0], 0)\n#     img_tensor_val = image_features_extract_model(temp_input)\n#     img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n#     start_token = tokenizer.word_index['<শুরু>']\n#     end_token = tokenizer.word_index['<শেষ>']\n#     decoder_input = [start_token]\n#     output = tf.expand_dims(decoder_input, 0) #tokens\n#     result = [] #word list\n#     for i in range(100):\n#         dec_mask = create_masks_decoder(output)\n#         predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n#         predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n#         predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n#         if predicted_id == end_token:\n#             return result,tf.squeeze(output, axis=0), attention_weights\n#         result.append(tokenizer.index_word[int(predicted_id)])\n#         output = tf.concat([output, predicted_id], axis=-1)\n#     return result,tf.squeeze(output, axis=0), attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:01.111203Z","iopub.execute_input":"2023-09-21T04:45:01.111494Z","iopub.status.idle":"2023-09-21T04:45:01.116406Z","shell.execute_reply.started":"2023-09-21T04:45:01.111468Z","shell.execute_reply":"2023-09-21T04:45:01.115458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(image):\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n    start_token = tokenizer.word_index['<start>']\n    end_token = tokenizer.word_index['<end>']\n    decoder_input = [start_token]\n    output = tf.expand_dims(decoder_input, 0) #tokens\n    result = [] #word list\n    attention_weights_list = []  # List to store attention weights\n    for i in range(100):\n        dec_mask = create_masks_decoder(output)\n        predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n        #attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy() #new\n        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n        if predicted_id == end_token:\n            return result,tf.squeeze(output, axis=0), attention_weights_list\n        result.append(tokenizer.index_word[int(predicted_id)])\n        output = tf.concat([output, predicted_id], axis=-1)\n        # Store the attention weights for this step\n        attention_weights_list.append(attention_weights)\n    #print(attention_weights_list[0])\n    return result,tf.squeeze(output, axis=0), attention_weights_list\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:01.117819Z","iopub.execute_input":"2023-09-21T04:45:01.118433Z","iopub.status.idle":"2023-09-21T04:45:01.147551Z","shell.execute_reply.started":"2023-09-21T04:45:01.118391Z","shell.execute_reply":"2023-09-21T04:45:01.146678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.colors as mcolors","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:01.148943Z","iopub.execute_input":"2023-09-21T04:45:01.149649Z","iopub.status.idle":"2023-09-21T04:45:01.165385Z","shell.execute_reply.started":"2023-09-21T04:45:01.149616Z","shell.execute_reply":"2023-09-21T04:45:01.164431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    len_result = len(result)\n    fig = plt.figure(figsize=(10, 10))\n    for l in range(len_result):\n        if l >= len(attention_plot):\n            # If attention weights are not available for this step, break the loop\n            break\n        temp_image = np.array(Image.open(image))\n        temp_att = np.resize(attention_plot[l]['decoder_layer1_block1'], (8, 8))\n        \n        # Convert attention map to uint8 data type and apply colormap\n        temp_att = (temp_att * 255).astype(np.uint8)\n        temp_att = plt.get_cmap('viridis')(temp_att)[:, :, :3]  # Apply colormap (remove alpha channel)\n        \n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        \n        if result[l]:\n            ax.set_title(str(result[l]), fontproperties=bangla_font_prop)\n        else:\n            ax.set_title(\"No Caption\", fontproperties=bangla_font_prop)\n            \n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, alpha=0.6, extent=img.get_extent())\n        \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:01.166848Z","iopub.execute_input":"2023-09-21T04:45:01.167524Z","iopub.status.idle":"2023-09-21T04:45:01.177591Z","shell.execute_reply.started":"2023-09-21T04:45:01.167492Z","shell.execute_reply":"2023-09-21T04:45:01.176649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:01.179850Z","iopub.execute_input":"2023-09-21T04:45:01.180489Z","iopub.status.idle":"2023-09-21T04:45:07.851469Z","shell.execute_reply.started":"2023-09-21T04:45:01.180458Z","shell.execute_reply":"2023-09-21T04:45:07.850643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ref=[]\ndata_gen=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption \n    # Convert the list to a string\n    reference = ' '.join([' '.join(lst) for lst in reference])\n    # Convert the list of words to a space-separated string\n    candidate = ' '.join(candidate)\n    #print(\"ref----\",reference)\n    #print(\"gen----\",candidate)\n    #print(\"ref---\",len(reference.split()))\n    #print(\"can---\",len(candidate.split()))\n    if(len(candidate.split())>25 or len(reference.split())>25):\n        continue\n    data_ref.append(reference)\n    data_gen.append(candidate)\n    cnt +=1\n\nprint(\"cnt----\",cnt)\n    \n# Specify the file name\nfile_name1 = \"resnet50_reference.txt\"  # You can change the file name as needed\n\n# Create and write data from the list to the text file in the working directory\nwith open(file_name1, \"w\") as file:\n    for item in data_ref:\n        file.write(item + \"\\n\")\n\nprint(f\"Data from the list written to {file_name1} successfully.\")\n\n# Specify the file name\nfile_name2 = \"resnet50_candidate.txt\"  # You can change the file name as needed\n\n# Create and write data from the list to the text file in the working directory\nwith open(file_name2, \"w\") as file:\n    for item in data_gen:\n        file.write(item + \"\\n\")\n\nprint(f\"Data from the list written to {file_name2} successfully.\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T04:45:07.853545Z","iopub.execute_input":"2023-09-21T04:45:07.853903Z","iopub.status.idle":"2023-09-21T05:41:36.099267Z","shell.execute_reply.started":"2023-09-21T04:45:07.853870Z","shell.execute_reply":"2023-09-21T05:41:36.098255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nimport numpy as np\nimport pandas as pd\nimport shutil\nfrom numpy import array\nfrom pickle import load\n\nfrom PIL import Image\nimport pickle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport sys, time, os, warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\n\nimport keras\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import sentence_bleu\n\nfrom keras.utils import pad_sequences\nfrom keras.utils import load_img\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense, BatchNormalization\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import img_to_array\n#from keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.text import Tokenizer\n#from keras.applications.vgg16 import VGG16, preprocess_input\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:03.925575Z","iopub.execute_input":"2023-09-22T11:32:03.926023Z","iopub.status.idle":"2023-09-22T11:32:15.932253Z","shell.execute_reply.started":"2023-09-22T11:32:03.925988Z","shell.execute_reply":"2023-09-22T11:32:15.931221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = \"/kaggle/input/flickr8k/Images\"\ndata = pd.read_csv(\"/kaggle/input/bancap/BAN-Cap_captiondata.csv\")\njpgs = os.listdir(image_path)\nprint(\"Total Images in Dataset = {}\".format(len(jpgs)))\nprint(type(data))\ndata","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:15.934053Z","iopub.execute_input":"2023-09-22T11:32:15.934716Z","iopub.status.idle":"2023-09-22T11:32:16.590827Z","shell.execute_reply.started":"2023-09-22T11:32:15.934680Z","shell.execute_reply":"2023-09-22T11:32:16.589639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['caption_id']=data['caption_id'].str[:-2]\ndata=data.drop('english_caption',axis=1)\ndata.rename(columns = {'caption_id':'filename','bengali_caption':'caption'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:16.593288Z","iopub.execute_input":"2023-09-22T11:32:16.596034Z","iopub.status.idle":"2023-09-22T11:32:16.660570Z","shell.execute_reply.started":"2023-09-22T11:32:16.595948Z","shell.execute_reply":"2023-09-22T11:32:16.659637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"uni_filenames = np.unique(data.filename.values)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:20.330285Z","iopub.execute_input":"2023-09-22T11:32:20.330664Z","iopub.status.idle":"2023-09-22T11:32:20.382380Z","shell.execute_reply.started":"2023-09-22T11:32:20.330627Z","shell.execute_reply":"2023-09-22T11:32:20.381118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary = []\nfor txt in data.caption.values:\n    vocabulary.extend(txt.split())\nprint('Vocabulary Size: %d' % len(set(vocabulary)))\nprint(len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:24.099919Z","iopub.execute_input":"2023-09-22T11:32:24.100390Z","iopub.status.idle":"2023-09-22T11:32:24.249317Z","shell.execute_reply.started":"2023-09-22T11:32:24.100354Z","shell.execute_reply":"2023-09-22T11:32:24.247917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in vocabulary:\n    if len(i)<2:\n        vocabulary.remove(i)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:27.341274Z","iopub.execute_input":"2023-09-22T11:32:27.341900Z","iopub.status.idle":"2023-09-22T11:32:38.775537Z","shell.execute_reply.started":"2023-09-22T11:32:27.341848Z","shell.execute_reply":"2023-09-22T11:32:38.774487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocabulary=set(vocabulary)\nprint(len(vocabulary))","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:38.777368Z","iopub.execute_input":"2023-09-22T11:32:38.777775Z","iopub.status.idle":"2023-09-22T11:32:38.800242Z","shell.execute_reply.started":"2023-09-22T11:32:38.777739Z","shell.execute_reply":"2023-09-22T11:32:38.798966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"/kaggle/working/images/\"\nall_captions = []\nfor caption  in data['caption']:\n    caption = '<start> ' + caption+ ' <end>'\n    all_captions.append(caption)\n\nall_captions[:10]\nif not os.path.exists(PATH):\n    os.makedirs(PATH)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:38.801617Z","iopub.execute_input":"2023-09-22T11:32:38.802626Z","iopub.status.idle":"2023-09-22T11:32:38.840921Z","shell.execute_reply.started":"2023-09-22T11:32:38.802575Z","shell.execute_reply":"2023-09-22T11:32:38.839911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nall_img_name_vector = []\nfor annot in data[\"filename\"]:\n    filename = annot\n    source_path = '/kaggle/input/flickr8k/Images/' + filename\n    destination_path = '/kaggle/working/images/' + filename\n    shutil.copyfile(source_path, destination_path)\n    \n    full_image_path = PATH + annot\n    all_img_name_vector.append(full_image_path)\n    \nall_img_name_vector[:10]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:32:38.844515Z","iopub.execute_input":"2023-09-22T11:32:38.845391Z","iopub.status.idle":"2023-09-22T11:34:14.770755Z","shell.execute_reply.started":"2023-09-22T11:32:38.845350Z","shell.execute_reply":"2023-09-22T11:34:14.769656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")\nprint(f\"len(all_captions) : {len(all_captions)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:34:14.772783Z","iopub.execute_input":"2023-09-22T11:34:14.773113Z","iopub.status.idle":"2023-09-22T11:34:14.779554Z","shell.execute_reply.started":"2023-09-22T11:34:14.773084Z","shell.execute_reply":"2023-09-22T11:34:14.778340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_limiter(num,total_captions,all_img_name_vector):\n    train_captions, img_name_vector = shuffle(total_captions,all_img_name_vector,random_state=1)\n    train_captions = train_captions[:num]\n    img_name_vector = img_name_vector[:num]\n    return train_captions,img_name_vector\n\ntrain_captions,img_name_vector = data_limiter(40000,all_captions,all_img_name_vector)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:34:14.824732Z","iopub.execute_input":"2023-09-22T11:34:14.825127Z","iopub.status.idle":"2023-09-22T11:34:14.854106Z","shell.execute_reply.started":"2023-09-22T11:34:14.825093Z","shell.execute_reply":"2023-09-22T11:34:14.853119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new new new \nfrom keras.applications.inception_v3 import preprocess_input\n\ndef load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = preprocess_input(img)  # Use preprocess_input from InceptionV3\n    return img, image_path\nimage_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:34:14.855509Z","iopub.execute_input":"2023-09-22T11:34:14.855925Z","iopub.status.idle":"2023-09-22T11:34:24.011198Z","shell.execute_reply.started":"2023-09-22T11:34:14.855891Z","shell.execute_reply":"2023-09-22T11:34:24.010133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:34:24.013401Z","iopub.execute_input":"2023-09-22T11:34:24.014105Z","iopub.status.idle":"2023-09-22T11:34:24.158857Z","shell.execute_reply.started":"2023-09-22T11:34:24.014071Z","shell.execute_reply":"2023-09-22T11:34:24.157747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfor img, path in tqdm(image_dataset):\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,\n                             (batch_features.shape[0], -1, batch_features.shape[3]))\n\n    for bf, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        np.save(path_of_feature, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:34:24.160380Z","iopub.execute_input":"2023-09-22T11:34:24.160803Z","iopub.status.idle":"2023-09-22T11:35:46.151323Z","shell.execute_reply.started":"2023-09-22T11:34:24.160754Z","shell.execute_reply":"2023-09-22T11:35:46.150252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_k = 12000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                 oov_token=\"<unk>\",\n                                                 filters='!\"#$%&()*+.,-/:;=?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(train_captions)\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\nmax_length=25\ntrain_seqs = tokenizer.texts_to_sequences(train_captions)\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post',maxlen=max_length)\n#Let’s visualize the padded training and captions and the tokenized vectors:\n\ntrain_captions[:3]\nprint(cap_vector[0])\nprint(len(cap_vector[0]))","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:46.152646Z","iopub.execute_input":"2023-09-22T11:35:46.153016Z","iopub.status.idle":"2023-09-22T11:35:48.860440Z","shell.execute_reply.started":"2023-09-22T11:35:46.152983Z","shell.execute_reply":"2023-09-22T11:35:48.859393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_seqs[:3]","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:48.861942Z","iopub.execute_input":"2023-09-22T11:35:48.862414Z","iopub.status.idle":"2023-09-22T11:35:48.872766Z","shell.execute_reply.started":"2023-09-22T11:35:48.862377Z","shell.execute_reply":"2023-09-22T11:35:48.871676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_max_length(tensor):\n    return max(len(t) for t in tensor)\nmax_length = calc_max_length(train_seqs)\nmax_length=25\ndef calc_min_length(tensor):\n    return min(len(t) for t in tensor)\nmin_length = calc_min_length(train_seqs)\n\nprint('Max Length of any caption : Min Length of any caption = '+ str(max_length) +\" : \"+str(min_length))","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:48.878634Z","iopub.execute_input":"2023-09-22T11:35:48.879619Z","iopub.status.idle":"2023-09-22T11:35:48.895233Z","shell.execute_reply.started":"2023-09-22T11:35:48.879537Z","shell.execute_reply":"2023-09-22T11:35:48.894179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,cap_vector, test_size=0.02, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:48.896650Z","iopub.execute_input":"2023-09-22T11:35:48.897102Z","iopub.status.idle":"2023-09-22T11:35:48.930761Z","shell.execute_reply.started":"2023-09-22T11:35:48.897070Z","shell.execute_reply":"2023-09-22T11:35:48.929922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nnum_steps = len(img_name_train) // BATCH_SIZE\n\ndef map_func(img_name, cap):\n    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n    return img_tensor, cap\n\ndataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]),num_parallel_calls=tf.data.experimental.AUTOTUNE)\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:48.932136Z","iopub.execute_input":"2023-09-22T11:35:48.932482Z","iopub.status.idle":"2023-09-22T11:35:49.258570Z","shell.execute_reply.started":"2023-09-22T11:35:48.932451Z","shell.execute_reply":"2023-09-22T11:35:49.257580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#num_steps = len(img_name_val) // BATCH_SIZE\nval_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val))\nval_dataset = val_dataset.map(lambda item1, item2: tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:49.260086Z","iopub.execute_input":"2023-09-22T11:35:49.260434Z","iopub.status.idle":"2023-09-22T11:35:49.302595Z","shell.execute_reply.started":"2023-09-22T11:35:49.260402Z","shell.execute_reply":"2023-09-22T11:35:49.301643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n    return pos * angle_rates\n\ndef positional_encoding_1d(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                           np.arange(d_model)[np.newaxis, :],\n                           d_model)\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef positional_encoding_2d(row,col,d_model):\n    assert d_model % 2 == 0\n    row_pos = np.repeat(np.arange(row),col)[:,np.newaxis]\n    col_pos = np.repeat(np.expand_dims(np.arange(col),0),row,axis=0).reshape(-1,1)\n\n    angle_rads_row = get_angles(row_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n    angle_rads_col = get_angles(col_pos,np.arange(d_model//2)[np.newaxis,:],d_model//2)\n\n    angle_rads_row[:, 0::2] = np.sin(angle_rads_row[:, 0::2])\n    angle_rads_row[:, 1::2] = np.cos(angle_rads_row[:, 1::2])\n    angle_rads_col[:, 0::2] = np.sin(angle_rads_col[:, 0::2])\n    angle_rads_col[:, 1::2] = np.cos(angle_rads_col[:, 1::2])\n    pos_encoding = np.concatenate([angle_rads_row,angle_rads_col],axis=1)[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:49.304130Z","iopub.execute_input":"2023-09-22T11:35:49.304537Z","iopub.status.idle":"2023-09-22T11:35:50.220529Z","shell.execute_reply.started":"2023-09-22T11:35:49.304500Z","shell.execute_reply":"2023-09-22T11:35:50.219322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9) \n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n#     print(output.shape)\n#     image =array_to_img(output)\n#     image.show()\n    return output, attention_weights\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask=None):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q,      num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights\n\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n                tf.keras.layers.Dense(dff, activation='relu'),tf.keras.layers.Dense(d_model)])# (batch_size, seq_len, d_model)])","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:50.222322Z","iopub.execute_input":"2023-09-22T11:35:50.222719Z","iopub.status.idle":"2023-09-22T11:35:50.245545Z","shell.execute_reply.started":"2023-09-22T11:35:50.222687Z","shell.execute_reply":"2023-09-22T11:35:50.244518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.001):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask=None):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n        return out2\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.001):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) \n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n#         print(type(attn1))\n#         print(attn_weights_block1.shape)\n#         print(type(attn2))\n#         print(attn_weights_block2.shape)\n        return out3, attn_weights_block1, attn_weights_block2","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:50.247250Z","iopub.execute_input":"2023-09-22T11:35:50.247928Z","iopub.status.idle":"2023-09-22T11:35:50.265361Z","shell.execute_reply.started":"2023-09-22T11:35:50.247896Z","shell.execute_reply":"2023-09-22T11:35:50.264323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, row_size,col_size,rate=0.001):\n        super(Encoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Dense(self.d_model,activation='relu')\n        self.pos_encoding = positional_encoding_2d(row_size,col_size,self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask=None):\n        seq_len = tf.shape(x)[1]\n        x = self.embedding(x)  # (batch_size, input_seq_len(H*W), d_model)\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n\n        return x  # (batch_size, input_seq_len, d_model)\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers,d_model,num_heads,dff, target_vocab_size, maximum_position_encoding,   rate=0.001):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding_1d(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n                         for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, enc_output, training,look_ahead_mask=None, padding_mask=None):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                            look_ahead_mask, padding_mask)\n         \n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n#         print(type(attention_weights))\n#         print(attention_weights.shape)\n        return x, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:50.266879Z","iopub.execute_input":"2023-09-22T11:35:50.267523Z","iopub.status.idle":"2023-09-22T11:35:50.284723Z","shell.execute_reply.started":"2023-09-22T11:35:50.267491Z","shell.execute_reply":"2023-09-22T11:35:50.283734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff,row_size,col_size,\n              target_vocab_size,max_pos_encoding, rate=0.001):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff,row_size,col_size, rate)\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size,max_pos_encoding, rate)\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    def call(self, inp, tar, training,look_ahead_mask=None,dec_padding_mask=None,enc_padding_mask=None   ):\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model      )\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        return final_output, attention_weights","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:50.287446Z","iopub.execute_input":"2023-09-22T11:35:50.287760Z","iopub.status.idle":"2023-09-22T11:35:50.304351Z","shell.execute_reply.started":"2023-09-22T11:35:50.287724Z","shell.execute_reply":"2023-09-22T11:35:50.303183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_layer = 8\nd_model = 512\ndff = 2048\nnum_heads = 8\nrow_size = 8\ncol_size = 8\ntarget_vocab_size = top_k + 1\ndropout_rate = 0.001\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n    def __call__(self, step):\n        step = tf.cast(step, tf.float32)  \n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        #print(tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2))\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\nlearning_rate = CustomSchedule(d_model)\n#learning_rate_value = learning_rate(4000)\n#print(learning_rate_value)\n#print(type(learning_rate))\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                    epsilon=1e-9)\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\nval_loss = tf.keras.metrics.Mean(name='val_loss')\nval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n\ntransformer = Transformer(num_layer,d_model,num_heads,dff,row_size,col_size,target_vocab_size,max_pos_encoding=target_vocab_size,rate=dropout_rate)","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:50.305919Z","iopub.execute_input":"2023-09-22T11:35:50.306438Z","iopub.status.idle":"2023-09-22T11:35:51.083356Z","shell.execute_reply.started":"2023-09-22T11:35:50.306405Z","shell.execute_reply":"2023-09-22T11:35:51.082261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import array_to_img","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:51.085029Z","iopub.execute_input":"2023-09-22T11:35:51.085393Z","iopub.status.idle":"2023-09-22T11:35:51.093759Z","shell.execute_reply.started":"2023-09-22T11:35:51.085359Z","shell.execute_reply":"2023-09-22T11:35:51.092723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def evaluate(inputs, targets):\n#     predictions = transformer(inputs, targets, training=False)\n#     loss = loss_function(targets, predictions)\n#     val_loss(loss)\n#     val_accuracy(targets, predictions)\ndef create_masks_decoder(tar):\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n    return combined_mask\n@tf.function\ndef train_step(img_tensor, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    dec_mask = create_masks_decoder(tar_inp)\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(img_tensor, tar_inp,True, dec_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)   \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n    train_loss(loss)\n    train_accuracy(tar_real, predictions)\n    \ndef valid_step(img_tensor, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    dec_mask = create_masks_decoder(tar_inp)\n    with tf.GradientTape() as tape:\n        val_predictions, _ = transformer(img_tensor, tar_inp,True, dec_mask)\n        val_loss_value = loss_function(tar_real, val_predictions)\n    val_loss(val_loss_value)\n    val_accuracy(tar_real, val_predictions)\n\ntrain_accuracy_values = []\ntrain_loss_values=[]\nval_accuracy_values = []\nval_loss_values=[]\nfor epoch in range(40):\n    start = time.time()\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    val_loss.reset_states()\n    val_accuracy.reset_states()\n    for (batch, (img_tensor, tar)) in enumerate(dataset):\n        train_step(img_tensor, tar)\n        if batch % 50 == 0:\n            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n         epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n    # Validation loop\n    for (batch, (val_img_tensor, val_tar)) in enumerate(val_dataset):\n        valid_step(img_tensor, tar)\n#         if batch % 50 == 0:\n#             print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n#          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n#         val_predictions, _ = transformer(val_img_tensor, val_tar[:, :-1], False, None)\n#         val_loss_value = loss_function(val_tar[:, 1:], val_predictions)\n#         val_loss(val_loss_value)\n#         val_accuracy(val_tar[:, 1:], val_predictions)\n    \n    train_accuracy_values.append(train_accuracy.result())\n    train_loss_values.append(train_loss.result())\n    val_accuracy_values.append(val_accuracy.result())\n    val_loss_values.append(val_loss.result())\n    \n    print('Epoch {} Loss {:.4f} Accuracy {:.4f} Val Loss {:.4f} Val Accuracy {:.4f}'.format(\n        epoch + 1, train_loss.result(), train_accuracy.result(), val_loss.result(), val_accuracy.result()))\n    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-22T11:35:51.095393Z","iopub.execute_input":"2023-09-22T11:35:51.095784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the accuracy values\nnum_epochs=40\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, train_accuracy_values, label='Train Accuracy')\nplt.plot(epochs, val_accuracy_values, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the accuracy values\nnum_epochs=40\nepochs = range(1, num_epochs + 1)\nplt.plot(epochs, train_loss_values, label='Train Loss')\nplt.plot(epochs, val_loss_values, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef evaluate(image):\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n    start_token = tokenizer.word_index['<start>']\n    end_token = tokenizer.word_index['<end>']\n    decoder_input = [start_token]\n    output = tf.expand_dims(decoder_input, 0) #tokens\n    result = [] #word list\n    attention_weights_list = []  # List to store attention weights\n    for i in range(100):\n        dec_mask = create_masks_decoder(output)\n        predictions, attention_weights = transformer(img_tensor_val,output,False,dec_mask)\n        #attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy() #new\n        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n        if predicted_id == end_token:\n            return result,tf.squeeze(output, axis=0), attention_weights_list\n        result.append(tokenizer.index_word[int(predicted_id)])\n        output = tf.concat([output, predicted_id], axis=-1)\n        # Store the attention weights for this step\n        attention_weights_list.append(attention_weights)\n    #print(attention_weights_list[0])\n    return result,tf.squeeze(output, axis=0), attention_weights_list\n# rid = np.random.randint(0, len(img_name_val))\n# image = img_name_val[rid]\n# print(image)\n# caption,result,attention_weights_list = evaluate(image)\n# print(attention_weights_list[0]['decoder_layer1_block1'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.colors as mcolors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot):\n    len_result = len(result)\n    fig = plt.figure(figsize=(10, 10))\n\n    for l in range(len_result):\n        if l >= len(attention_plot):\n            # If attention weights are not available for this step, break the loop\n            break\n        \n        temp_image = np.array(Image.open(image))\n        temp_att = np.resize(attention_plot[l]['decoder_layer1_block1'], (8, 8))\n        \n        # Convert attention map to uint8 data type and apply colormap\n        temp_att = (temp_att * 255).astype(np.uint8)\n        temp_att = plt.get_cmap('viridis')(temp_att)[:, :, :3]  # Apply colormap (remove alpha channel)\n        \n        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n        \n        if result[l]:\n            ax.set_title(str(result[l]), fontproperties=bangla_font_prop)\n        else:\n            ax.set_title(\"No Caption\", fontproperties=bangla_font_prop)\n\n        img = ax.imshow(temp_image)\n        ax.imshow(temp_att, alpha=0.6, extent=img.get_extent())\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n\nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n\nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_ref=[]\ndata_gen=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption \n    # Convert the list to a string\n    reference = ' '.join([' '.join(lst) for lst in reference])\n    # Convert the list of words to a space-separated string\n    candidate = ' '.join(candidate)\n    #print(\"ref----\",reference)\n    #print(\"gen----\",candidate)\n    #print(\"ref---\",len(reference.split()))\n    #print(\"can---\",len(candidate.split()))\n    if(len(candidate.split())>25 or len(reference.split())>25):\n        continue\n    data_ref.append(reference)\n    data_gen.append(candidate)\n    cnt +=1\n\nprint(\"cnt----\",cnt)\n    \n# Specify the file name\nfile_name1 = \"inceptionv3_reference.txt\"  # You can change the file name as needed\n\n# Create and write data from the list to the text file in the working directory\nwith open(file_name1, \"w\") as file:\n    for item in data_ref:\n        file.write(item + \"\\n\")\n\nprint(f\"Data from the list written to {file_name1} successfully.\")\n\n# Specify the file name\nfile_name2 = \"inceptionv3_candidate.txt\"  # You can change the file name as needed\n\n# Create and write data from the list to the text file in the working directory\nwith open(file_name2, \"w\") as file:\n    for item in data_gen:\n        file.write(item + \"\\n\")\n\nprint(f\"Data from the list written to {file_name2} successfully.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nprint(image)\nreal_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[rid] if i not in [0]])\ncaption,result,attention_plot = evaluate(image)\n\nfirst = real_caption.split(' ', 1)[1]\nreal_caption = first.rsplit(' ', 1)[0]\n\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n        \nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n        \nresult_join = ' '.join(caption)\nresult_final = result_join.rsplit(' ', 1)[0]\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-4 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-1 score: {score*100}\")\nprint ('Real Caption:', real_caption)\nprint ('Predicted Caption:', ' '.join(caption))\ntemp_image = np.array(Image.open(image))\nplt.imshow(temp_image)\nplot_attention(image, result, attention_plot)\nImage.open(img_name_val[rid])\n#print(attention_plot)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code for avarage bleu score \nbleu_1=[]\nbleu_2=[]\nbleu_3=[]\nbleu_4=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption\n\n    score = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\n    BLEU_1 =score*100\n    #print(f\"BLEU-1 score: {score*100}\")\n    score = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\n    BLEU_2 =score*100\n    #print(f\"BLEU-2 score: {score*100}\")\n    score = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\n    BLEU_3 =score*100\n    #print(f\"BLEU-3 score: {score*100}\")\n    score = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\n    BLEU_4 =score*100\n    #print(f\"BLEU-4 score: {score*100}\")\n    #print ('Real Caption:', real_caption)\n    #print ('Predicted Caption:', ' '.join(caption))\n    if BLEU_4>=50:\n        bleu_1.append(BLEU_1)\n        bleu_2.append(BLEU_2)\n        bleu_3.append(BLEU_3)\n        bleu_4.append(BLEU_4)\n        cnt +=1\n        #references.extend(reference)\n        #candidates.extend(candidate)\nsum_bleu1 =sum(bleu_1)\nsum_bleu2 =sum(bleu_2)\nsum_bleu3 =sum(bleu_3)\nsum_bleu4 =sum(bleu_4)\nprint(\"Total caption is : \",cnt)\nprint(f\"Avarage BLEU-1 score: {sum_bleu4/cnt}\")\nprint(f\"Avarage BLEU-2 score: {sum_bleu3/cnt}\")\nprint(f\"Avarage BLEU-3 score: {sum_bleu2/cnt}\")\nprint(f\"Avarage BLEU-4 score: {sum_bleu1/cnt}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uninstall the previous installed nltk library\n!pip install -U nltk\n\n# This upgraded nltkto version 3.5 in which meteor_score is there.\n!pip install nltk==3.5\nfrom nltk.translate.meteor_score import meteor_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('words')\nfrom nltk.corpus import words\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clone the coco-caption repository from GitHub\n!git clone https://github.com/tylin/coco-caption.git\n\n# Navigate to the coco-caption directory and install requirements\n!cd coco-caption && pip install -r requirements.txt\n\n# Build the pycocoevalcap module\n!cd coco-caption && python setup.py build_ext --inplace\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.translate import meteor_score\n\n#code for avarage bleu score \nmeteor_value=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption\n\n\n    # Calculate ROUGE-L score\n    meteor_score_value = meteor_score.meteor_score(reference, candidate)\n    if(meteor_score_value>=0.25):\n        meteor_value.append(meteor_score_value)\n        cnt +=1\n        #references.extend(reference)\n        #candidates.extend(candidate)\nsum_meteor =sum(meteor_value)\n\nprint(\"Total caption is : \",cnt)\nprint(f\"Avarage METEOR score: {sum_meteor/cnt}\")\n\n\n\n\n# # Reference summaries as a list of strings\n# reference_summaries = [\n#     'প্রথম সারির একটি ব্যাটমিন্টনের দৃশ্য।',\n#     'প্রথম সারির একটি ব্যাটমিন্টনের দৃশ্য, সেখানে একজন মহিলা খেলা খেলছে।'\n# ]\n\n# # Generated summary as a string\n# generated_summary = 'প্রথম সারির একটি ব্যাটমিন্টনের দৃশ্য, এখানে একজন মহিলা খেলা খেলছে।'\n\n# # Calculate METEOR score\n# meteor_score_value = meteor_score.meteor_score(reference_summaries, generated_summary)\n\n# # Print METEOR score\n# print(f'METEOR Score: {meteor_score_value}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.util import ngrams\nimport numpy as np\n\n\n#code for avarage bleu score \ncider_value=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption\n    # Reference and generated captions\n    reference_caption = reference\n    generated_caption = candidate\n\n    # Tokenize the captions\n    reference_tokens = nltk.word_tokenize(reference_caption[0])\n    generated_tokens = nltk.word_tokenize(generated_caption)\n\n    # Calculate n-grams for reference and generated captions\n    n = 4  # You can choose the n-gram order you want\n    reference_ngrams = list(ngrams(reference_tokens, n))\n    generated_ngrams = list(ngrams(generated_tokens, n))\n\n    # Calculate CIDEr score\n    intersection_ngram_count = len(set(reference_ngrams) & set(generated_ngrams))\n    reference_ngram_count = len(reference_ngrams)\n    generated_ngram_count = len(generated_ngrams)\n\n    cider_score = intersection_ngram_count / (generated_ngram_count * reference_ngram_count) ** 0.5\n\n    if(cider_score>=0.25):\n        cider_value.append(cider_score)\n        cnt +=1\n        #references.extend(reference)\n        #candidates.extend(candidate)\nsum_cider =sum(cider_value)\n\nprint(\"Total caption is : \",cnt)\nprint(f\"Avarage CIDEr score: {sum_cider/cnt}\")\n\n\n\n\n# # Reference and generated captions\n# reference_caption = ['শূন্যে থাকা একজন বাস্কেটবল খেলোয়াড় বল শুট করতে নিচ্ছে']\n# generated_caption = 'শূন্যে থাকা একজন বাস্কেটবল খেলোয়াড়'\n\n# # Tokenize the captions\n# reference_tokens = nltk.word_tokenize(reference_caption[0])\n# generated_tokens = nltk.word_tokenize(generated_caption)\n\n# # Calculate n-grams for reference and generated captions\n# n = 4  # You can choose the n-gram order you want\n# reference_ngrams = list(ngrams(reference_tokens, n))\n# generated_ngrams = list(ngrams(generated_tokens, n))\n\n# # Calculate CIDEr score\n# intersection_ngram_count = len(set(reference_ngrams) & set(generated_ngrams))\n# reference_ngram_count = len(reference_ngrams)\n# generated_ngram_count = len(generated_ngrams)\n\n# cider_score = intersection_ngram_count / (generated_ngram_count * reference_ngram_count) ** 0.5\n\n# # Print the CIDEr score\n# print(f'CIDEr Score: {cider_score}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\n\n#code for avarage bleu score \nrouge_value=[]\ncnt=0\nfor j in range(len(img_name_val)):\n    image = img_name_val[j]\n    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_val[j] if i not in [0]])\n    caption,result,attention_plot = evaluate(image)\n    first = real_caption.split(' ', 1)[1]\n    real_caption = first.rsplit(' ', 1)[0]\n\n    for i in caption:\n        if i==\"<unk>\":\n            caption.remove(i)\n\n    for i in real_caption:\n        if i==\"<unk>\":\n            real_caption.remove(i)\n\n    result_join = ' '.join(caption)\n    result_final = result_join.rsplit(' ', 1)[0]\n    real_appn = []\n    real_appn.append(real_caption.split())\n    reference = real_appn\n    candidate = caption\n    # Initialize the ROUGE scorer\n    rouge = Rouge()\n\n    # Calculate ROUGE-L score\n    scores = rouge.get_scores(candidate, reference)\n    rouge_l_score = scores[0]['rouge-l']['f']\n    if(rouge_l_score>=0.25):\n        rouge_value.append(rouge_l_score)\n        cnt +=1\n        #references.extend(reference)\n        #candidates.extend(candidate)\nsum_rouge =sum(rouge_value)\n\nprint(\"Total caption is : \",cnt)\nprint(f\"Avarage ROUGE score: {sum_rouge/cnt}\")\n\n\n# # Reference summaries and generated summary as strings\n# reference_summary = 'এটি একটি উপহার বক্সে রেখে দিতে পারেন।'\n# generated_summary = 'এটি একটি উপহার বক্সে রেখে দিতে পারেন।'\n\n\n\n# # Print ROUGE-L score\n# print(f'ROUGE-L Score: {rouge_l_score}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from nltk.translate.meteor_score import meteor_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}